{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e29377",
   "metadata": {},
   "source": [
    "# Advanced Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5370dbb",
   "metadata": {},
   "source": [
    "Note: this lab has been tested with Python 3.10. We recommend using the same Python version if there are problems with libraries used in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d2eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generated in W6 Lab or the provided data splits (see Absalon, W7 Lab)\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_pickle(\"train_dataframe.pkl\")\n",
    "df_test = pd.read_pickle(\"test_dataframe.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ee73231",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "For this exercise, you can use the Python library Scikit-Surprise. Please find the documentation here: https://surprise.readthedocs.io/en/stable/getting_started.html.\n",
    "\n",
    "Define an SVD model with user and item biases that uses Stochastic Gradient Descend (SGD) to estimate the low-rank matrix based on only observed ratings.\n",
    "\n",
    "Fit the model on the full training set with $30$ latent factors and $100$ epochs. Keep Scikit-Surprise's default setting for all other parameters, but set the random state to $0$ for comparable results.\n",
    "\n",
    "Use the model to predict the unobserved ratings for the users in the training set. How many predictions are there and what is the average of all the predictions? Round the average of all predictions to the third decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9f14ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions:  54746\n",
      "Average of predictions:  4.413\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "\n",
    "# Convert train data format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_matrix = Dataset.load_from_df(df_train[['reviewerID', 'asin', 'overall']], reader)\n",
    "\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "trainset = training_matrix.build_full_trainset()\n",
    "testset = trainset.build_anti_testset()\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD(n_factors=30, n_epochs=100)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(training_matrix.build_full_trainset())\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "print(\"Number of predictions: \", len(predictions))\n",
    "print(\"Average of predictions: \", round(np.mean([pred.est for pred in predictions]), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d0c69",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "We will implement the Neural Matrix Factorization model using the Python library RecBole.\n",
    "Please find the documentation here: https://recbole.io/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1879e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: recbole in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (2.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (1.15.1)\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (4.67.1)\n",
      "Requirement already satisfied: colorlog==4.7.2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (4.7.2)\n"
     ]
    }
   ],
   "source": [
    "#Uncomment and run the following line if you need to install RecBole\n",
    "!pip install recbole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama==0.4.4 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (0.4.4)Requirement already satisfied: scikit-learn>=0.23.2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (1.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (6.0.2)\n",
      "Requirement already satisfied: tensorboard>=2.5.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (2.19.0)\n",
      "Requirement already satisfied: thop>=0.1.1.post2207130030 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (0.9.0)\n",
      "Requirement already satisfied: plotly>=4.0.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (6.0.0)\n",
      "Requirement already satisfied: texttable>=0.9.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from recbole) (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from pandas>=1.4.0->recbole) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from pandas>=1.4.0->recbole) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from pandas>=1.4.0->recbole) (2025.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from plotly>=4.0.0->recbole) (1.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from plotly>=4.0.0->recbole) (24.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-learn>=0.23.2->recbole) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-learn>=0.23.2->recbole) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from tensorboard>=2.5.0->recbole) (3.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from torch>=1.10.0->recbole) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from torch>=1.10.0->recbole) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from torch>=1.10.0->recbole) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from torch>=1.10.0->recbole) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from torch>=1.10.0->recbole) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from torch>=1.10.0->recbole) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->recbole) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.5.0->recbole) (3.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: frozenlist in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from ray) (1.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from ray) (2.32.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from click>=7.0->ray) (0.4.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from jsonschema->ray) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from jsonschema->ray) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from jsonschema->ray) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from jsonschema->ray) (0.23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from requests->ray) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from requests->ray) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from requests->ray) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from requests->ray) (2025.1.31)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from referencing>=0.28.4->jsonschema->ray) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "#Uncomment and run the following line if you need to install ray. This is needed when calling run_recbole\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b20d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from recbole.quick_start import run_recbole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49bbb1",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Convert the dataset to the format which can be read by RecBole.\n",
    "\n",
    "More information regarding the input data format can be found here:\n",
    "https://recbole.io/docs/user_guide/usage/running_new_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfcbae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are creating a dictionary that maps the column names in our dataset to the column names required by RecBole\n",
    "\n",
    "#Fill this dictionary with keys that are column names in our dataset that correspond to user_id, item_id, rating, and timestamp\n",
    "#Fill the values of the dictionary according to the given documentation\n",
    "\n",
    "col_name_dict = {\n",
    "    'reviewerID': 'user_id:token',\n",
    "    'asin': 'item_id:token',\n",
    "    'overall': 'rating:float',\n",
    "    'unixReviewTime': 'timestamp:float'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "570e0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method converts a dataframe to a .inter file, and saves it in the folder \"data\" under the name 'file_name'\n",
    "def convert_df_to_inter(df:pd.DataFrame, col_name_dict:dict, file_name:str):\n",
    "    inter = df.copy()\n",
    "    selected_cols = col_name_dict.keys()\n",
    "    inter = inter[selected_cols]\n",
    "\n",
    "    # Rename the columns in inter using col_name_dict\n",
    "    inter.rename(columns=col_name_dict, inplace=True)\n",
    "\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "    inter.to_csv(\"data/\"+file_name, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23315418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an extra, empty dataframe with the same column names in the keys of col_name_dict\n",
    "#we will use this as a dummy validation file\n",
    "df_extra = pd.DataFrame(columns=col_name_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a639e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_df_to_inter(df_train, col_name_dict, \"data.train.inter\")\n",
    "convert_df_to_inter(df_test, col_name_dict, \"data.test.inter\")\n",
    "convert_df_to_inter(df_extra, col_name_dict, \"data.extra.inter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe5a09",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Train the Neural Matrix Factorization model on the whole training dataset for $100$ epochs.\n",
    "\n",
    "Evaluate the model on the test set, based on HR, MRR, Precision, MAP, and Recall at $k \\in \\{5, 10, 20\\}$ respectively and round the scores up to 3 decimal places (It is fine if you have different results in the third decimal point).\n",
    "Keep the rest of the default settings of RecBole the same.\n",
    "\n",
    "Note: RecBole's MAP normalises the recall base by $\\min\\{k,G\\}$, where $G$ is the recall base (see W7 lecture and homework solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a63a97",
   "metadata": {},
   "source": [
    "Note: A non exhaustive list of properties that can be set using the config_dict parameter of the run_recbole() method can be found here:\n",
    "https://github.com/RUCAIBox/RecBole/blob/master/recbole/properties/overall.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c443c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22 Feb 17:45    INFO  ['c:\\\\Users\\\\david\\\\anaconda3\\\\envs\\\\WRS\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '--f=c:\\\\Users\\\\david\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3852bff4147a4d113379f72de5ea977b8ac703ff1.json']\n",
      "22 Feb 17:45    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = ./data\n",
      "checkpoint_dir = saved\n",
      "show_progress = False\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 100\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}\n",
      "repeatable = False\n",
      "metrics = ['Hit', 'MRR', 'Precision', 'MAP', 'Recall']\n",
      "topk = [5, 10, 20]\n",
      "valid_metric = MRR@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 3\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = ['train', 'extra', 'test']\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "mf_embedding_size = 64\n",
      "mlp_embedding_size = 64\n",
      "mlp_hidden_size = [128, 64]\n",
      "dropout_prob = 0.1\n",
      "mf_train = True\n",
      "mlp_train = True\n",
      "use_pretrain = False\n",
      "mf_pretrain_path = None\n",
      "mlp_pretrain_path = None\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cpu\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "c:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "22 Feb 17:45    INFO  data\n",
      "The number of users: 982\n",
      "Average actions of users: 4.161060142711519\n",
      "The number of items: 85\n",
      "Average actions of items: 48.595238095238095\n",
      "The number of inters: 4082\n",
      "The sparsity of the dataset: 95.10962022283455%\n",
      "Remain Fields: ['user_id', 'item_id']\n",
      "22 Feb 17:45    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "22 Feb 17:45    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "22 Feb 17:45    INFO  NeuMF(\n",
      "  (user_mf_embedding): Embedding(982, 64)\n",
      "  (item_mf_embedding): Embedding(85, 64)\n",
      "  (user_mlp_embedding): Embedding(982, 64)\n",
      "  (item_mlp_embedding): Embedding(85, 64)\n",
      "  (mlp_layers): MLPLayers(\n",
      "    (mlp_layers): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (predict_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (loss): BCEWithLogitsLoss()\n",
      ")\n",
      "Trainable parameters: 161473\n",
      "22 Feb 17:45    INFO  FLOPs: 24960.0\n",
      "c:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\recbole\\trainer\\trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
      "22 Feb 17:45    INFO  epoch 0 training [time: 0.05s, train loss: 2.7716]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 1 training [time: 0.03s, train loss: 2.7666]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 2 training [time: 0.03s, train loss: 2.7552]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 3 training [time: 0.03s, train loss: 2.7279]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 4 training [time: 0.03s, train loss: 2.6818]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 5 training [time: 0.03s, train loss: 2.6137]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 6 training [time: 0.03s, train loss: 2.5118]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 7 training [time: 0.03s, train loss: 2.3985]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 8 training [time: 0.03s, train loss: 2.2303]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 9 training [time: 0.05s, train loss: 2.0398]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 10 training [time: 0.04s, train loss: 1.7527]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 11 training [time: 0.03s, train loss: 1.5393]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 12 training [time: 0.03s, train loss: 1.1944]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 13 training [time: 0.03s, train loss: 0.9783]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 14 training [time: 0.03s, train loss: 0.8025]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 15 training [time: 0.03s, train loss: 0.7134]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 16 training [time: 0.06s, train loss: 0.6235]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 17 training [time: 0.03s, train loss: 0.6060]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 18 training [time: 0.04s, train loss: 0.5639]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 19 training [time: 0.03s, train loss: 0.6488]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 20 training [time: 0.03s, train loss: 0.6105]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 21 training [time: 0.03s, train loss: 0.5157]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 22 training [time: 0.04s, train loss: 0.5157]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 23 training [time: 0.03s, train loss: 0.5260]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 24 training [time: 0.03s, train loss: 0.5606]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 25 training [time: 0.03s, train loss: 0.5760]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 26 training [time: 0.04s, train loss: 0.4841]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 27 training [time: 0.03s, train loss: 0.4883]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 28 training [time: 0.03s, train loss: 0.5083]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 29 training [time: 0.06s, train loss: 0.5027]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 30 training [time: 0.03s, train loss: 0.4487]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 31 training [time: 0.03s, train loss: 0.4306]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 32 training [time: 0.03s, train loss: 0.5251]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 33 training [time: 0.04s, train loss: 0.4672]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 34 training [time: 0.03s, train loss: 0.4614]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 35 training [time: 0.03s, train loss: 0.4708]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 36 training [time: 0.03s, train loss: 0.4447]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 37 training [time: 0.03s, train loss: 0.3750]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 38 training [time: 0.03s, train loss: 0.3849]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 39 training [time: 0.03s, train loss: 0.4093]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 40 training [time: 0.03s, train loss: 0.4210]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 41 training [time: 0.04s, train loss: 0.3584]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 42 training [time: 0.03s, train loss: 0.3640]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 43 training [time: 0.03s, train loss: 0.4130]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 44 training [time: 0.07s, train loss: 0.3649]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 45 training [time: 0.03s, train loss: 0.3697]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 46 training [time: 0.03s, train loss: 0.3759]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 47 training [time: 0.03s, train loss: 0.3246]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 48 training [time: 0.03s, train loss: 0.3753]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 49 training [time: 0.03s, train loss: 0.2936]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 50 training [time: 0.03s, train loss: 0.3436]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 51 training [time: 0.03s, train loss: 0.3557]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 52 training [time: 0.03s, train loss: 0.3804]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 53 training [time: 0.03s, train loss: 0.4217]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 54 training [time: 0.03s, train loss: 0.2929]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 55 training [time: 0.03s, train loss: 0.3004]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 56 training [time: 0.05s, train loss: 0.2659]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 57 training [time: 0.03s, train loss: 0.2676]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 58 training [time: 0.03s, train loss: 0.2687]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 59 training [time: 0.03s, train loss: 0.3130]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 60 training [time: 0.03s, train loss: 0.2385]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 61 training [time: 0.03s, train loss: 0.2713]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 62 training [time: 0.03s, train loss: 0.2828]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 63 training [time: 0.03s, train loss: 0.2571]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 64 training [time: 0.04s, train loss: 0.2985]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 65 training [time: 0.03s, train loss: 0.2598]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 66 training [time: 0.07s, train loss: 0.3304]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 67 training [time: 0.04s, train loss: 0.2815]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 68 training [time: 0.03s, train loss: 0.2577]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 69 training [time: 0.03s, train loss: 0.3037]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 70 training [time: 0.03s, train loss: 0.2625]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 71 training [time: 0.03s, train loss: 0.2404]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 72 training [time: 0.03s, train loss: 0.2828]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 73 training [time: 0.03s, train loss: 0.2335]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 74 training [time: 0.03s, train loss: 0.2899]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 75 training [time: 0.05s, train loss: 0.2353]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 76 training [time: 0.04s, train loss: 0.2304]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 77 training [time: 0.03s, train loss: 0.2305]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 78 training [time: 0.03s, train loss: 0.2233]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 79 training [time: 0.03s, train loss: 0.2116]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 80 training [time: 0.03s, train loss: 0.2160]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 81 training [time: 0.04s, train loss: 0.2651]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 82 training [time: 0.03s, train loss: 0.2311]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 83 training [time: 0.03s, train loss: 0.2051]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 84 training [time: 0.04s, train loss: 0.2192]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 85 training [time: 0.04s, train loss: 0.2011]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 86 training [time: 0.03s, train loss: 0.2595]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 87 training [time: 0.04s, train loss: 0.2232]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 88 training [time: 0.03s, train loss: 0.1895]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 89 training [time: 0.03s, train loss: 0.1957]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 90 training [time: 0.03s, train loss: 0.2185]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 91 training [time: 0.03s, train loss: 0.2267]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 92 training [time: 0.03s, train loss: 0.1688]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 93 training [time: 0.06s, train loss: 0.1820]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 94 training [time: 0.03s, train loss: 0.2143]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 95 training [time: 0.03s, train loss: 0.1819]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 96 training [time: 0.03s, train loss: 0.1922]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 97 training [time: 0.03s, train loss: 0.1785]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 98 training [time: 0.03s, train loss: 0.1860]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  epoch 99 training [time: 0.03s, train loss: 0.1847]\n",
      "22 Feb 17:45    INFO  Saving current: saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "c:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\recbole\\trainer\\trainer.py:583: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=self.device)\n",
      "22 Feb 17:45    INFO  Loading model structure and parameters from saved\\NeuMF-Feb-22-2025_17-45-41.pth\n",
      "22 Feb 17:45    INFO  The running environment of this training is as follows:\n",
      "+-------------+----------------+\n",
      "| Environment |     Usage      |\n",
      "+=============+================+\n",
      "| CPU         |    13.30 %     |\n",
      "+-------------+----------------+\n",
      "| GPU         |   0.0 / 0.0    |\n",
      "+-------------+----------------+\n",
      "| Memory      | 0.46 G/15.40 G |\n",
      "+-------------+----------------+\n",
      "22 Feb 17:45    INFO  best valid : None\n",
      "22 Feb 17:45    INFO  test result: OrderedDict([('hit@5', 0.812), ('hit@10', 0.819), ('hit@20', 0.857), ('mrr@5', 0.533), ('mrr@10', 0.534), ('mrr@20', 0.536), ('precision@5', 0.162), ('precision@10', 0.082), ('precision@20', 0.043), ('map@5', 0.533), ('map@10', 0.534), ('map@20', 0.536), ('recall@5', 0.812), ('recall@10', 0.819), ('recall@20', 0.857)])\n"
     ]
    }
   ],
   "source": [
    "result = run_recbole(\n",
    "                    model=\"NeuMF\",\n",
    "                    dataset=\"data\",\n",
    "                    config_dict={\n",
    "                        \"data_path\":\"./\",\n",
    "                        \"benchmark_filename\": ['train', 'extra', 'test'],\n",
    "                        \"topk\": [5, 10, 20],\n",
    "                        \"metric_decimal_place\": 3,\n",
    "                        \"metrics\": [\"Hit\", \"MRR\", \"Precision\", \"MAP\", \"Recall\"],\n",
    "                        \"epochs\": 100,\n",
    "                        \"show_progress\": False\n",
    "                    })\n",
    "\n",
    "#print the results here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb09d0",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Let's create a graph-based recommender system, defining neighbourhoods with random walks. Build a bipartite graph (i.e., edges only between users and items) where nodes are users and items; a **bidirectional** edge $(u,i)$ exists in the graph if user $u$ has rated item $i$ with a score $>3$. \n",
    "\n",
    "Implement the Page Rank algorithm to find the top-10 recommended items for user `ARARUVZ8RUF5T`. You can use the `pagerank` method from the library `networkx`. Assume a damping factor of $0.85$ and leave the rest of parameters by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85f407c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aad322ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (73650615.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    df_convert = #get the rows in the df where the rating is >3\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "\n",
    "# Prepare the data\n",
    "def convert_data(df):\n",
    "    df_convert = #get the rows in the df where the rating is >3\n",
    "    df_convert = df_convert[[\"asin\",\"reviewerID\"]]\n",
    "    df_convert_arr = df_convert.values\n",
    "    return df_convert_arr\n",
    "\n",
    "train_df = convert_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975926df",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Hyper Parameters '''\n",
    "def parameter_dict_from_vector(vector):\n",
    "    return {\n",
    "        \"W_USER_ITEM\" : vector[0],\n",
    "        \"W_USER_ITEM_BACK\" : vector[1]\n",
    "        }\n",
    "\n",
    "''' Building Graph '''\n",
    "class InteractionGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.MultiDiGraph()\n",
    "        \n",
    "    def add_nodes_from_edge_array(self, edge_array, type_1, type_2):\n",
    "        nodes = [(x[0], {'type': type_1}) for x in edge_array] \\\n",
    "        + [(x[1], {'type': type_2}) for x in edge_array]\n",
    "        self.graph.add_nodes_from(nodes)\n",
    "\n",
    "    def add_edges_from_array(self, array, weight_front=1.0, weight_back=1.0):\n",
    "        forward_edges = [(x[0], x[1], weight_front) for x in array]\n",
    "        back_edges = [(x[1], x[0], weight_back) for x in array]\n",
    "        self.graph.add_weighted_edges_from(forward_edges)\n",
    "        self.graph.add_weighted_edges_from(back_edges)\n",
    "\n",
    "def build_graph(parameter_dictionary, user_item_array):\n",
    "    multigraph = InteractionGraph()\n",
    "    multigraph.add_nodes_from_edge_array(user_item_array, 'item', 'user')\n",
    "    multigraph.add_edges_from_array(user_item_array, \n",
    "                                    parameter_dictionary[\"W_USER_ITEM\"], \n",
    "                                    parameter_dictionary[\"W_USER_ITEM_BACK\"])\n",
    "    return multigraph\n",
    "\n",
    "class RecommendationEngine:\n",
    "    def __init__(self, multigraph, damping_factor = 0.3):\n",
    "        self.graph = nx.DiGraph()\n",
    "        for u,v,d in multigraph.graph.edges(data=True):\n",
    "            w = d['weight']\n",
    "            if self.graph.has_edge(u,v):\n",
    "                self.graph[u][v]['weight'] += w\n",
    "            else:\n",
    "                self.graph.add_edge(u,v,weight=w)\n",
    "        self.nodes = list(self.graph.nodes)\n",
    "        self.damping_factor = damping_factor\n",
    "        \n",
    "        #this part keeps track of items that have been rated by each user in the training set\n",
    "        self.user_item_dict = {}\n",
    "        for n in multigraph.graph.nodes.data():\n",
    "            if n[1]['type'] == 'user':\n",
    "                 self.user_item_dict[n[0]] = set()\n",
    "        for e in multigraph.graph.edges:\n",
    "            if e[0] in self.user_item_dict:\n",
    "                 self.user_item_dict[e[0]].add(e[1])\n",
    "\n",
    "    def generate_pr(self, damping_factor):\n",
    "        pr = #use the pagerank method here\n",
    "        pr_sorted = dict(\n",
    "            #sort pr by descending probability values\n",
    "            )\n",
    "        pr_list = [(k, v) for k, v in pr_sorted.items()]\n",
    "        return pr_list\n",
    "    \n",
    "    def generate_recommendations(self, user):\n",
    "        pr_list = self.generate_pr(self.damping_factor)\n",
    "        result = #Given the user, remove items in their recommendation list that they have rated in the training set\n",
    "        #hint: you can use user_item_dict for this\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = #fill this\n",
    "\n",
    "# Build the graph\n",
    "graph = build_graph(parameter_dict_from_vector(np.ones(2)), train_df)\n",
    "# Build the recommender system with page rank\n",
    "recommender = RecommendationEngine(graph, damping_factor)\n",
    "\n",
    "# Get top-K recommendations for the given user \n",
    "user_id = \"ARARUVZ8RUF5T\"\n",
    "K = 10\n",
    "\n",
    "# write your code to get the top-K recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd76a50",
   "metadata": {},
   "source": [
    "Credits: the provided codes in Exercise 3 are modified from\n",
    "https://arxiv.org/pdf/2301.11009.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WRS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
