{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d23701c",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcface96",
   "metadata": {},
   "source": [
    "Based on the same dataset used on previous weeks, let us evaluate the Collaborative Filtering (CF) model implemented last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160a7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits from Week 6, the files are also uploaded in Absalon\n",
    "import pandas as pd \n",
    "train_df = pd.read_pickle(\"train_dataframe.pkl\") \n",
    "test_df = pd.read_pickle(\"test_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a97e87",
   "metadata": {},
   "source": [
    "Recall that `reviewerID` corresponds to user, `asin` corresponds to item, and `overall` is the user-given rating to the item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5b50d",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Based on the user-based neighborhood model that was created last week, let's make a general system that can be used to generate recommendations for all users and items. The system would take into account the mean rating of each user. We can use Scikit-Surprise for this.\n",
    "https://surprise.readthedocs.io/en/stable/index.html\n",
    "\n",
    "Use cosine as similarity measure and try to vary the (maximum) number of neighbors to take into account when predicting ratings. Set the random state to $0$ for comparable results. Keep Scikit-Surprise's default settings for all other parameters. \n",
    "\n",
    "Is it better to use $1$ or $10$ neighbors? You should determine this based on the Root Mean Square Error (RMSE) over 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554581e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-surprise) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-surprise) (1.15.1)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run the following line if you need to install scikit-surprise, note that this library is not the same as sklearn\n",
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8916aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert train data format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_matrix = Dataset.load_from_df(train_df[['reviewerID', 'asin', 'overall']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6aa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fix the random seed\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "# 3. Define a cross-validation iterator\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "rmse_result = dict()\n",
    "\n",
    "list_neighbour = [1, 10]\n",
    "for neighbour in list_neighbour:\n",
    "    algo = KNNWithMeans(k=neighbour,\n",
    "                        sim_options={\"name\":\"cosine\",\"user_based\":True},\n",
    "                        verbose=False,\n",
    "                        random_state=0)\n",
    "    rmse_result[neighbour] = {}\n",
    "    \n",
    "    fold = 0\n",
    "    for trainset, testset in kf.split(training_matrix):\n",
    "\n",
    "        # train and test algorithm.\n",
    "        algo.fit(trainset)\n",
    "        \n",
    "        predictions_KNN = algo.test(testset)\n",
    "        df_pred_KNN = pd.DataFrame(predictions_KNN)\n",
    "\n",
    "        actual_ratings = df_pred_KNN['r_ui']\n",
    "        predicted_ratings = df_pred_KNN['est']\n",
    "        rmse_result[neighbour][fold] = np.sqrt(mse(actual_ratings, predicted_ratings))\n",
    "\n",
    "        fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbff5869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest average RMSE: 0.4356721705776638\n",
      "Number of neighbors with lowest validation RMSE: 10\n"
     ]
    }
   ],
   "source": [
    "# Convert the RMSE results dictionary to a DataFrame\n",
    "df_rmse = pd.DataFrame(rmse_result)\n",
    "\n",
    "# Compute the average RMSE across folds for each neighbor\n",
    "avg_rmse_per_neighbor = df_rmse.mean()\n",
    "\n",
    "# Find the neighbor with the lowest average RMSE\n",
    "best_neighbor = avg_rmse_per_neighbor.idxmin()\n",
    "\n",
    "print(\"Lowest average RMSE:\", avg_rmse_per_neighbor.min())\n",
    "print('Number of neighbors with lowest validation RMSE:', best_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5f8a7",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "### 2.1\n",
    "Fit the neigborhood-based model defined in exercise 1 on the full training set with cosine as similarity measure and either $1$ or $10$ neighbors based on what you found to be better in exercise 1. Keep Scikit-Surprise's default settings for all other parameters, but set the random state to $0$ for comparable results.\n",
    "\n",
    "Use the model to predict the unobserved ratings for the users in the training set. Remove predictions for users that are not in the test set (`test_df`).\n",
    "\n",
    "How many predictions are there and what is the average of all the predictions (rounded to 2 decimal places)?\n",
    "\n",
    "*Note:* there may be items in the test set that are not present in the training set; these items are not included in counting the number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d9e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 52988\n",
      "Average prediction: 4.73\n"
     ]
    }
   ],
   "source": [
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True\n",
    "               }\n",
    "algo = KNNWithMeans(k= 10,\n",
    "                    sim_options=sim_options, \n",
    "                    random_state=0, \n",
    "                    verbose=False)\n",
    "\n",
    "train_data = training_matrix.build_full_trainset()\n",
    "algo.fit(train_data)\n",
    "\n",
    "unobserved_ratings = train_data.build_anti_testset()\n",
    "pred_KNN = algo.test(unobserved_ratings)\n",
    "\n",
    "# Detect users from training set that are not in test\n",
    "test_users = set(test_df['reviewerID'])\n",
    "\n",
    "# Filter predictions: keep only those for users in the test set.\n",
    "filtered_preds = [pred.est for pred in pred_KNN if pred.uid in test_users]\n",
    "\n",
    "# Get the number of predictions and the average value rounded to 2 decimals.\n",
    "num_predictions = len(filtered_preds)\n",
    "avg_prediction = round(sum(filtered_preds) / num_predictions, 2) if num_predictions else None\n",
    "\n",
    "print(\"Number of predictions:\", num_predictions)\n",
    "print(\"Average prediction:\", avg_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b35f40",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Report the RMSE of the rating prediction of users and items in `test_df` (rounded to 3 decimal places).\n",
    "\n",
    "Note that the documentation https://surprise.readthedocs.io/en/stable/predictions_module.html defines `r_ui` as the true rating of user $u$ for item $i$, but this can be somewhat misleading, as it depends on the input. If you run the prediction based on the anti-testset of the training set, then it won't have access to the true rating and instead use the mean rating of all users over all items, which then subsequently lands in the prediction class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d3e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual values shape: (830,)\n",
      "Predicted values shape: (830,)\n",
      "Test RMSE: 0.295\n"
     ]
    }
   ],
   "source": [
    "df_pred_KNN = pd.DataFrame(pred_KNN)\n",
    "\n",
    "# Merge test_df and df_pred_KNN on the corresponding key columns using an inner join.\n",
    "merged_df = pd.merge(\n",
    "    test_df,                                        # complete test set\n",
    "    df_pred_KNN,                                    # predictions as DataFrame\n",
    "    left_on=['reviewerID', 'asin'],                 # keys from test set\n",
    "    right_on=['uid', 'iid'],                        # keys from predictions\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Sort by the keys if needed\n",
    "merged_df = merged_df.sort_values(by=[\"reviewerID\", \"asin\"]).reset_index(drop=True)\n",
    "\n",
    "# Extract actual and predicted ratings from the merged DataFrame.\n",
    "actual_vals = merged_df[\"overall\"]\n",
    "est_vals = merged_df[\"est\"]\n",
    "\n",
    "print(f\"Actual values shape: {actual_vals.shape}\")\n",
    "print(f\"Predicted values shape: {est_vals.shape}\")\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_value = np.sqrt(mse(actual_vals, est_vals))\n",
    "print(f\"Test RMSE: {rmse_value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf3c25",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Define a general method to get the top-k recommendations for each user, based on the rating predictions obtained in Exercise 2.1.\n",
    "\n",
    "Print the top-k with $k=\\{5, 10, 20\\}$ recommendations for the user with ID `ARARUVZ8RUF5T` and its estimated ratings. Round the printed estimated ratings to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c95e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from surprise.prediction_algorithms.predictions import Prediction\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "def get_top_k(predictions: List[Prediction], \n",
    "              k: int) -> Dict[str, List]:\n",
    "    \"\"\"Compute the top-K recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "    topk = defaultdict(list)\n",
    "\n",
    "    # Sort first by uid, then by est in descending order\n",
    "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
    "\n",
    "    # Extract the top k predictions per user\n",
    "    for pred in sorted_predictions:\n",
    "        if len(topk[pred.uid]) < k:  # Ensure only top-k per user\n",
    "            topk[pred.uid].append((pred.iid, pred.est))\n",
    "\n",
    "    return topk\n",
    "\n",
    "def print_top_k(user_id: str, topk: Dict[str, List]) -> None:\n",
    "    user_ratings = topk[user_id]\n",
    "    print(f\"TOP-{len(user_ratings)} predictions for user {user_id}: {[(item, round(rating,2)) for (item, rating) in user_ratings]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6a2ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-5 predictions for user ARARUVZ8RUF5T: [('B000WR2HB6', 5), ('B000FOI48G', 4.68), ('B000VV1YOY', 4.67), ('B001ET7FZE', 4.6), ('B000PKKAGO', 4.5)]\n",
      "TOP-10 predictions for user ARARUVZ8RUF5T: [('B000WR2HB6', 5), ('B000FOI48G', 4.68), ('B000VV1YOY', 4.67), ('B001ET7FZE', 4.6), ('B000PKKAGO', 4.5), ('B00EF1QRMU', 4.47), ('B016V8YWBC', 4.46), ('B00W259T7G', 4.42), ('B00CZH3K1C', 4.33), ('B000GLRREU', 4.23)]\n",
      "TOP-20 predictions for user ARARUVZ8RUF5T: [('B000WR2HB6', 5), ('B000FOI48G', 4.68), ('B000VV1YOY', 4.67), ('B001ET7FZE', 4.6), ('B000PKKAGO', 4.5), ('B00EF1QRMU', 4.47), ('B016V8YWBC', 4.46), ('B00W259T7G', 4.42), ('B00CZH3K1C', 4.33), ('B000GLRREU', 4.23), ('B00N2WQ2IW', 4.22), ('B00EYZY6LQ', 4.2), ('B01BNEYGQU', 4.17), ('B002GP80EU', 4.04), ('B0009RF9DW', 4.0), ('B000FI4S1E', 4.0), ('B000URXP6E', 4.0), ('B00006L9LC', 4.0), ('B0012Y0ZG2', 4.0), ('B001OHV1H4', 4.0)]\n"
     ]
    }
   ],
   "source": [
    "for k in [5, 10, 20]:\n",
    "    topk = get_top_k(pred_KNN, k)\n",
    "    print_top_k(\"ARARUVZ8RUF5T\", topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325a14",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Report Precision@k (P@k), MAP@k and the MRR@k with $k=\\{5, 10, 20\\}$ averaged across users for the CF model. Round the scores to 3 decimal places. When computing P@k and MAP@k, we consider as relevant items those with an observed rating $\\geq 4.0$ (i.e., those items from the test set with a rating $\\geq$ 4.0). Thus, in this exercise, if a user receives an item that is present in the user’s test split, the item is considered relevant since the test split only contains items with ratings $\\geq 4.0$. Reflect on the differences obtained between the metrics and the different cut-off $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d813afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def precision_at_k(predictions: List[Prediction], \n",
    "                   df_test: pd.DataFrame,\n",
    "                   k: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute precision at k for each user\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        A dict where keys are user ids (str)\n",
    "        and values are the P@k (float) for each of them\n",
    "    \"\"\"\n",
    "\n",
    "    precisions = defaultdict(float)\n",
    "    \n",
    "    # First map the predictions to each user.\n",
    "    topk = get_top_k(predictions, k)\n",
    "\n",
    "    # Cycle for each key in the top-k dictionary\n",
    "    for user_id, raccomendation in topk.items():\n",
    "\n",
    "        # Get the actual items the user has rated in the test set\n",
    "        relevant_items = df_test[(df_test['reviewerID'] == user_id) & (df_test['overall'] >= 4)]['asin'].tolist()\n",
    "\n",
    "        # Count the number of hits in the top-k list:\n",
    "        num_hits = sum([1 for (item, _) in raccomendation if item in relevant_items])\n",
    "        \n",
    "        # Compute the precisiosn at k\n",
    "        precisions[user_id] = num_hits / k\n",
    "\n",
    "    return precisions\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(predictions: List[Prediction], \n",
    "                           df_test: pd.DataFrame,\n",
    "                           k: int) -> float:\n",
    "    \"\"\"Compute the mean average precision \n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        The MAP@k (float)\n",
    "    \"\"\"\n",
    "\n",
    "    average_precision_users = []\n",
    "    \n",
    "    for user_id, recommendations in topk.items():\n",
    "        relevant_items = df_test[(df_test['reviewerID'] == user_id) & (df_test['overall'] >= 4)]['asin'].tolist()\n",
    "        if not relevant_items:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        num_hits = 0\n",
    "        for i, (item, _) in enumerate(recommendations, start=1):\n",
    "            if item in relevant_items:\n",
    "                num_hits += 1\n",
    "                score += num_hits / i\n",
    "        if num_hits > 0:\n",
    "            average_precision_users.append(score / min(len(relevant_items), k))\n",
    "        else:\n",
    "            average_precision_users.append(0.0)\n",
    "    \n",
    "    mapk = np.mean(average_precision_users)\n",
    "    return mapk\n",
    "    \n",
    "\n",
    "def mean_reciprocal_rank(predictions: List[Prediction], \n",
    "                         df_test: pd.DataFrame, \n",
    "                         k) -> float:\n",
    "    \"\"\"Compute the mean reciprocal rank \n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        The MRR@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    reciprocal_rank = []\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    mean_rr = np.mean(reciprocal_rank)\n",
    "    return mean_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9711402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Neighborhood based CF:\n",
      "Averaged P@5: 0.143\n",
      "MAP@5: 0.174\n",
      "MRR@5: nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "precision_at_k() missing 1 required positional argument: 'df_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMRR@5: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mrr_nb))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# PRECISION\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m precisions_nb \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_at_k\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;66;43;03m# Complete, \u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAveraged P@10: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28msum\u001b[39m(prec \u001b[38;5;28;01mfor\u001b[39;00m prec \u001b[38;5;129;01min\u001b[39;00m precisions_nb\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(precisions_nb)))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# MAP \u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: precision_at_k() missing 1 required positional argument: 'df_test'"
     ]
    }
   ],
   "source": [
    "# -------- NB BASED --------\n",
    "print(\"Metrics for Neighborhood based CF:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(pred_KNN, \n",
    "    test_df, k=5)\n",
    "print(\"Averaged P@5: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(pred_KNN, \n",
    "    test_df, k=5)\n",
    "print(\"MAP@5: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(pred_KNN, \n",
    "    test_df, k=5)\n",
    "print(\"MRR@5: {:.3f}\".format(mrr_nb))\n",
    "\n",
    "\n",
    "\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "\n",
    "\n",
    "\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=20)\n",
    "print(\"Averaged P@20: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=20)\n",
    "print(\"MAP@20: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=20)\n",
    "print(\"MRR@20: {:.3f}\".format(mrr_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c50b",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Based on the top-5, top-10 and top-20 predictions from Exercise 3, compute the system’s hit rate averaged over the total number of users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba976c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(top_k: Dict[str, List[str]],\n",
    "             df_test: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute the hit rate\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n (output of get_top_k())\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "    Returns:\n",
    "        The average hit rate\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    return hits\n",
    "\n",
    "print(\"Hit Rate for Neighborhood based CF:\")\n",
    "print(\"Hit Rate (top-5): {:.3f}\".format(hit_rate( #Complete )))\n",
    "print(\"Hit Rate (top-10): {:.3f}\".format(hit_rate( #Complete )))\n",
    "print(\"Hit Rate (top-20): {:.3f}\".format(hit_rate( #Complete )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WRS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
