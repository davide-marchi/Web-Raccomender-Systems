{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d23701c",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcface96",
   "metadata": {},
   "source": [
    "Based on the same dataset used on previous weeks, let us evaluate the Collaborative Filtering (CF) model implemented last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "160a7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data splits from Week 6, the files are also uploaded in Absalon\n",
    "import pandas as pd \n",
    "train_df = pd.read_pickle(\"train_dataframe.pkl\") \n",
    "test_df = pd.read_pickle(\"test_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a97e87",
   "metadata": {},
   "source": [
    "Recall that `reviewerID` corresponds to user, `asin` corresponds to item, and `overall` is the user-given rating to the item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5b50d",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Based on the user-based neighborhood model that was created last week, let's make a general system that can be used to generate recommendations for all users and items. The system would take into account the mean rating of each user. We can use Scikit-Surprise for this.\n",
    "https://surprise.readthedocs.io/en/stable/index.html\n",
    "\n",
    "Use cosine as similarity measure and try to vary the (maximum) number of neighbors to take into account when predicting ratings. Set the random state to $0$ for comparable results. Keep Scikit-Surprise's default settings for all other parameters. \n",
    "\n",
    "Is it better to use $1$ or $10$ neighbors? You should determine this based on the Root Mean Square Error (RMSE) over 3-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "554581e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-surprise) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\david\\anaconda3\\envs\\wrs\\lib\\site-packages (from scikit-surprise) (1.15.1)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run the following line if you need to install scikit-surprise, note that this library is not the same as sklearn\n",
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e3426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8916aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert train data format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_matrix = Dataset.load_from_df(train_df[['reviewerID', 'asin', 'overall']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c6aa4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fix the random seed\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "# 3. Define a cross-validation iterator\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "rmse_result = dict()\n",
    "\n",
    "list_neighbour = [1, 10]\n",
    "for neighbour in list_neighbour:\n",
    "    algo = KNNWithMeans(k=neighbour,\n",
    "                        sim_options={\"name\":\"cosine\",\"user_based\":True},\n",
    "                        verbose=False,\n",
    "                        random_state=0)\n",
    "    rmse_result[neighbour] = {}\n",
    "    \n",
    "    fold = 0\n",
    "    for trainset, testset in kf.split(training_matrix):\n",
    "\n",
    "        # train and test algorithm.\n",
    "        algo.fit(trainset)\n",
    "        \n",
    "        predictions_KNN = algo.test(testset)\n",
    "        df_pred_KNN = pd.DataFrame(predictions_KNN)\n",
    "\n",
    "        actual_ratings = df_pred_KNN['r_ui']\n",
    "        predicted_ratings = df_pred_KNN['est']\n",
    "        rmse_result[neighbour][fold] = np.sqrt(mse(actual_ratings, predicted_ratings))\n",
    "\n",
    "        fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbff5869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest average RMSE: 0.4356721705776638\n",
      "Number of neighbors with lowest validation RMSE: 10\n"
     ]
    }
   ],
   "source": [
    "# Convert the RMSE results dictionary to a DataFrame\n",
    "df_rmse = pd.DataFrame(rmse_result)\n",
    "\n",
    "# Compute the average RMSE across folds for each neighbor\n",
    "avg_rmse_per_neighbor = df_rmse.mean()\n",
    "\n",
    "# Find the neighbor with the lowest average RMSE\n",
    "best_neighbor = avg_rmse_per_neighbor.idxmin()\n",
    "\n",
    "print(\"Lowest average RMSE:\", avg_rmse_per_neighbor.min())\n",
    "print('Number of neighbors with lowest validation RMSE:', best_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5f8a7",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "### 2.1\n",
    "Fit the neigborhood-based model defined in exercise 1 on the full training set with cosine as similarity measure and either $1$ or $10$ neighbors based on what you found to be better in exercise 1. Keep Scikit-Surprise's default settings for all other parameters, but set the random state to $0$ for comparable results.\n",
    "\n",
    "Use the model to predict the unobserved ratings for the users in the training set. Remove predictions for users that are not in the test set (`test_df`).\n",
    "\n",
    "How many predictions are there and what is the average of all the predictions (rounded to 2 decimal places)?\n",
    "\n",
    "*Note:* there may be items in the test set that are not present in the training set; these items are not included in counting the number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1d9e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 52988\n",
      "Average prediction: 4.73\n"
     ]
    }
   ],
   "source": [
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True\n",
    "               }\n",
    "algo = KNNWithMeans(k= 10,\n",
    "                    sim_options=sim_options, \n",
    "                    random_state=0, \n",
    "                    verbose=False)\n",
    "\n",
    "train_data = training_matrix.build_full_trainset()\n",
    "algo.fit(train_data)\n",
    "\n",
    "unobserved_ratings = train_data.build_anti_testset()\n",
    "pred_KNN = algo.test(unobserved_ratings)\n",
    "\n",
    "# Detect users from training set that are not in test\n",
    "test_users = set(test_df['reviewerID'])\n",
    "\n",
    "# Filter predictions: keep only those for users in the test set.\n",
    "filtered_preds = [pred.est for pred in pred_KNN if pred.uid in test_users]\n",
    "\n",
    "# Get the number of predictions and the average value rounded to 2 decimals.\n",
    "num_predictions = len(filtered_preds)\n",
    "avg_prediction = round(sum(filtered_preds) / num_predictions, 2) if num_predictions else None\n",
    "\n",
    "print(\"Number of predictions:\", num_predictions)\n",
    "print(\"Average prediction:\", avg_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b35f40",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Report the RMSE of the rating prediction of users and items in `test_df` (rounded to 3 decimal places).\n",
    "\n",
    "Note that the documentation https://surprise.readthedocs.io/en/stable/predictions_module.html defines `r_ui` as the true rating of user $u$ for item $i$, but this can be somewhat misleading, as it depends on the input. If you run the prediction based on the anti-testset of the training set, then it won't have access to the true rating and instead use the mean rating of all users over all items, which then subsequently lands in the prediction class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49d3e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949,)\n",
      "(830,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [949, 830]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(est_vals\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Compute RMSE\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m rmse_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest_vals\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#rmse_value = sqrt(mean_squared_error(actual_vals, est_vals))\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#print(f\"Test RMSE: {rmse_value:.3f}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\sklearn\\metrics\\_regression.py:565\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03m0.825...\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    563\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[0;32m    564\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 565\u001b[0m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m )\n\u001b[0;32m    569\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    570\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\sklearn\\metrics\\_regression.py:198\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mregression task.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 198\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\sklearn\\metrics\\_regression.py:104\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 104\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    106\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\WRS\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [949, 830]"
     ]
    }
   ],
   "source": [
    "df_pred_KNN = pd.DataFrame(pred_KNN)\n",
    "\n",
    "#print(df_pred_KNN)\n",
    "\n",
    "\n",
    "#print(test_df)\n",
    "\n",
    "\n",
    "\n",
    "pairs_test = set(zip(test_df['reviewerID'], test_df['asin']))\n",
    "df_pred_KNN = df_pred_KNN[df_pred_KNN.apply(lambda row: (row[\"uid\"], row[\"iid\"]) in pairs_test, axis=1)]\n",
    "\n",
    "# Sort both dataframes by user/item so that rows align\n",
    "df_pred_KNN = df_pred_KNN.sort_values(by=[\"uid\", \"iid\"]).reset_index(drop=True)\n",
    "test_df = test_df.sort_values(by=[\"reviewerID\", \"asin\"]).reset_index(drop=True)\n",
    "\n",
    "# Extract actual (overall) and predicted (est) ratings\n",
    "actual_vals = test_df[\"overall\"]\n",
    "est_vals = df_pred_KNN[\"est\"]\n",
    "\n",
    "print(actual_vals.shape)\n",
    "print(est_vals.shape)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_value = np.sqrt(mse(actual_vals, est_vals))\n",
    "print(f\"Test RMSE: {rmse_value:.3f}\")\n",
    "\n",
    "#rmse_value = sqrt(mean_squared_error(actual_vals, est_vals))\n",
    "#print(f\"Test RMSE: {rmse_value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf3c25",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Define a general method to get the top-k recommendations for each user, based on the rating predictions obtained in Exercise 2.1.\n",
    "\n",
    "Print the top-k with $k=\\{5, 10, 20\\}$ recommendations for the user with ID `ARARUVZ8RUF5T` and its estimated ratings. Round the printed estimated ratings to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from surprise.prediction_algorithms.predictions import Prediction\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "def get_top_k(predictions: List[Prediction], \n",
    "              k: int) -> Dict[str, List]:\n",
    "    \"\"\"Compute the top-K recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "    topk = defaultdict(list)\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    return topk\n",
    "\n",
    "def print_top_k(user_id: str, topk: Dict[str, List]) -> None:\n",
    "    user_ratings = topk[user_id]\n",
    "    print(f\"TOP-{len(user_ratings)} predictions for user {user_id}: {[(item, round(rating,2)) for (item, rating) in user_ratings]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325a14",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Report Precision@k (P@k), MAP@k and the MRR@k with $k=\\{5, 10, 20\\}$ averaged across users for the CF model. Round the scores to 3 decimal places. When computing P@k and MAP@k, we consider as relevant items those with an observed rating $\\geq 4.0$ (i.e., those items from the test set with a rating $\\geq$ 4.0). Thus, in this exercise, if a user receives an item that is present in the user’s test split, the item is considered relevant since the test split only contains items with ratings $\\geq 4.0$. Reflect on the differences obtained between the metrics and the different cut-off $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d813afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def precision_at_k(predictions: List[Prediction], \n",
    "                   df_test: pd.DataFrame,\n",
    "                   k: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute precision at k for each user\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        A dict where keys are user ids (str)\n",
    "        and values are the P@k (float) for each of them\n",
    "    \"\"\"\n",
    "\n",
    "    precisions = defaultdict(float)\n",
    "    \n",
    "    # First map the predictions to each user.\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    return precisions\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(predictions: List[Prediction], \n",
    "                           df_test: pd.DataFrame,\n",
    "                           k: int) -> float:\n",
    "    \"\"\"Compute the mean average precision \n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        The MAP@k (float)\n",
    "    \"\"\"\n",
    "\n",
    "    average_precision_users = []\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    mapk = np.mean(average_precision_users)\n",
    "    return mapk\n",
    "    \n",
    "\n",
    "def mean_reciprocal_rank(predictions: List[Prediction], \n",
    "                         df_test: pd.DataFrame, \n",
    "                         k) -> float:\n",
    "    \"\"\"Compute the mean reciprocal rank \n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        The MRR@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    reciprocal_rank = []\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    mean_rr = np.mean(reciprocal_rank)\n",
    "    return mean_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9711402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- NB BASED --------\n",
    "print(\"Metrics for Neighborhood based CF:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=5)\n",
    "print(\"Averaged P@5: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=5)\n",
    "print(\"MAP@5: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=5)\n",
    "print(\"MRR@5: {:.3f}\".format(mrr_nb))\n",
    "\n",
    "\n",
    "\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "\n",
    "\n",
    "\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(# Complete, \n",
    "    test_df, k=20)\n",
    "print(\"Averaged P@20: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(# Complete, \n",
    "    test_df, k=20)\n",
    "print(\"MAP@20: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(# Complete, \n",
    "    test_df, k=20)\n",
    "print(\"MRR@20: {:.3f}\".format(mrr_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c50b",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Based on the top-5, top-10 and top-20 predictions from Exercise 3, compute the system’s hit rate averaged over the total number of users in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba976c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(top_k: Dict[str, List[str]],\n",
    "             df_test: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute the hit rate\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n (output of get_top_k())\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "    Returns:\n",
    "        The average hit rate\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    \n",
    "    # Write your code here\n",
    "    \n",
    "    return hits\n",
    "\n",
    "print(\"Hit Rate for Neighborhood based CF:\")\n",
    "print(\"Hit Rate (top-5): {:.3f}\".format(hit_rate( #Complete )))\n",
    "print(\"Hit Rate (top-10): {:.3f}\".format(hit_rate( #Complete )))\n",
    "print(\"Hit Rate (top-20): {:.3f}\".format(hit_rate( #Complete )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WRS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
